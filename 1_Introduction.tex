%! TEX root = optimization17.tex

\section{Introduction}\label{sec:intro}
\emph{Speaker: Patrick Rebeschini, 12/10/2017.}\\

\label{sec:introduction}

Many problems in statistics and machine learning can be formulated as the problem of computing
\begin{equation}
	x^\star \in \argmin_{x\in\mathcal{X}} r(x),
	\label{def:mainproblem}
\end{equation}
where $r(x):=\mathbf{E}\ell(x^T\Phi(W),Y)$, given only data in the form of $m$ i.i.d.\ labeled samples $(W_1,Y_1),\ldots,(W_m,Y_m) \sim (W,Y) \in \mathcal{W}\times\mathcal{Y}$, i.e., without knowledge of the distribution of $(W,Y)$ (in particular, without knowledge of the function $r$ that we want to minimize!). Here, the function $\Phi:\mathcal{W}\rightarrow\mathbb{R}^n$ (known) is a feature map, the function $x \in \mathcal{X}\subseteq\mathbb{R}^n \rightarrow x^T\Phi(W)$ is the linear predictor for the label $Y$ ($\mathcal{X}$ is known),
%where the parameter space $\mathcal{X}\subseteq\mathbb{R}^n$ possibly also includes regularization constraints, 
the function $\ell:\mathbb{R}\times \mathbb{R} \rightarrow \mathbb{R}$ (known) is a loss function characterizing the penalty in committing a wrong prediction, and $r(x)$ is the \emph{test cost}  or \emph{expected risk} associated to the parameter $x$. In this setting, one wants to find $x^\star$ that yields the best linear predictor over unseen data using the data in the training set, relying on the assumption that the unseen data share the same (unknown) distribution of observed samples.

In binary classification, one has $\mathcal{Y}=\{-1,1\}$ and loss functions are typically of the form $\ell(z,y) = \varphi(-zy)$. A few examples are:
\begin{itemize}
\item Zero-One loss (a.k.a.\ the true loss): $\varphi(u) = \mathbf{1}_{u\ge 0}$.
\item Exponential loss: $\varphi(u) = \exp(u)$.
\item Hinge loss: $\varphi(u) = \max\{0,1+u\}$ (giving SVM).
\item Logistic loss: $\varphi(u) = \log_2(1+\exp(u))$.
\end{itemize}
In regression, one has $\mathcal{Y}=\mathbb{R}$, and the typical loss is:
\begin{itemize}
\item Least-squares loss: $\ell(u,y) = (u-y)^2$.
\end{itemize}

Perhaps the most intuitive paradigm for solving problem \eqref{def:mainproblem} is the \emph{empirical risk minimization}, that is, the idea to minimize the \emph{training cost} or \emph{empirical risk}:
$$
	R(x) := \frac{1}{m} \sum_{i=1}^m R_i(x),
$$
where $R_i(x) := \ell(x^T\Phi(W_i),Y_i)$.
This approach is motivated by the fact that for each $x\in\mathcal{X}$ the law of large number gives
$
	r(x) \approx R(x)
$
so that one might expect that
$
	x^\star \approx X^\star,
$
where
$
	X^\star \in \argmin_{x\in\mathcal{X}} R(x).
$
This intuition can be made precise, and Proposition \ref{prop:erm} below shows that the estimation error $r(X^\star) - r(x^\star) \ge 0$ can be controlled by suprema of random processes.

\begin{proposition}
\label{prop:erm}
We have
$$
	r(X^\star) - r(x^\star)
	\le
	\sup_{x\in\mathcal{X}} ( r(x) - R(x) ) + \sup_{x\in\mathcal{X}} ( R(x) - r(x) ).
$$
\end{proposition}

\begin{proof}
By adding and subtracting terms, we get
\begin{align*}
	r(X^\star) - r(x^\star)
	= r(X^\star) - R(X^\star) + R(X^\star) - R(x^\star) + R(x^\star) - r(x^\star).
\end{align*}
As $X^\star$ is a minimizer of $R$, we have $R(X^\star) - R(x^\star) \le 0$. The proof follows by taking the supremum over $x\in\mathcal{X}$.
\end{proof}

This analysis assumes that one can actually compute an exact minimizer $X^\star$ for the training cost $R$. With a finite amount of computational resources available, however, typically one can only compute an approximate minimizer. Let $\hat X_t\in\mathcal{X}$ denote an approximate solution to $\argmin_{x\in\mathcal{X}} R(x)$ that is computed after $t$ iterations of a given algorithmic procedure. Proceeding as above we have the following result.

\begin{proposition}
\label{prop:erm2}
We have
\begin{align}
	r(\hat X_t) - r(x^\star)
	\le
	\underbrace{\sup_{x\in\mathcal{X}} ( r(x) - R(x) ) + \sup_{x\in\mathcal{X}} ( R(x) - r(x) )}_{\textrm{STATISTICS}}
	+
	\underbrace{R(\hat X_t) - R(X^\star)}_{\textrm{OPTIMIZATION}}.
	\label{bound:stats-opt}
\end{align}
\end{proposition}

\begin{proof}
By adding and subtracting terms, we get
\begin{align*}
	r(\hat X_t) - r(x^\star)
	= r(\hat X_t) - R(\hat X_t) + R(\hat X_t) - R(X^\star) + R(X^\star) - R(x^\star) + R(x^\star) - r(x^\star).
\end{align*}
As $X^\star$ is a minimizer of $R$, we have $R(X^\star) - R(x^\star) \le 0$. The proof follows by taking the supremum over $x\in\mathcal{X}$.
\end{proof}

The problem is now to estimate how close the expected risk of the approximate empirical risk minimizer $r(\hat X_t)$ is to the minimal expected risk $r(x^\star)$ in terms of the number of training samples $m$, the dimension of the feature map $n$, the number of iterates $t$ for the optimization routine, and the other parameters that define the model at hand (for example, the radius of the parameter set $\mathcal{X}$, properties of the loss function $\ell$, and of the feature map $\Phi$).
To this end, as Proposition \ref{prop:erm2} shows, we need to control two terms: the \emph{STATISTICS} term $\sup_{x\in\mathcal{X}} ( r(x) - R(x) ) + \sup_{x\in\mathcal{X}} ( R(x) - r(x) )$ and the \emph{OPTIMIZATION} term $R(\hat X_t) - R(X^\star)$.

In the first part of the reading group we will focus on algorithms that can be used to approximately minimize the empirical risk and hence control the \emph{OPTIMIZATION} term in \eqref{bound:stats-opt}, making various assumptions for the loss function $\ell$ and the parameter space $\mathcal{X}$, starting from the fruitful assumption of convexity.\footnote{convexity is only needed to perform the analysis of the algorithms that we introduce, but it is not needed to run the algorithms themselves; in fact, in practice the algorithms that we define are run also on non-convex problems.} Along with convexity, we state some of the main properties that a generic function $f$ can have that are used to assess the quality of algorithms to minimize $f$. Here we assume that $f$ is differentiable, but analogous notions can be defined for non-differentiable functions using the notion of subgradients. We also mention in brackets equivalent (local) definitions, which apart from $L$-Lipschitz hold when $f$ is twice-differentiable.

\begin{itemize}
	\item Convexity: $f(x) - f(y) \le \nabla f(x)^T(x - y)$ for any $x,y\in\mathbb{R}^n$ ($\nabla^2 f(x) \succcurlyeq 0$ for any $x\in\mathbb{R}^n$).
	\item $L$-Lipschitz: $| f(x) - f(y) | \le L \| x - y \|_2$ for any $x,y\in\mathbb{R}^n$ ($\|\nabla f(x)\|_2 \le L$ for any $x\in\mathbb{R}^n$).
	\item $\beta$-Smoothness: $\| \nabla f(x) - \nabla f(y) \|_2 \le \beta \| x - y \|_2$ for any $x,y\in\mathbb{R}^n$ ($\nabla^2 f(x) \preccurlyeq \beta I$ for any $x\in\mathbb{R}^n$).
	\item $\alpha$-Strong convexity: $f(x) - f(y) \le \nabla f(x)^T(x - y) - \frac{\alpha}{2}\|x - y\|_2^2$ for any $x,y\in\mathbb{R}^n$ ($\nabla^2 f(x) \succcurlyeq \alpha I$ for any $x\in\mathbb{R}^n$).
\end{itemize}

Going back to the losses defined above, it is easy to check that the following holds:
\begin{itemize}
\item Exponential loss: $\varphi(u) = \exp(u)$: Lipschitz NO, Smooth NO, strongly-convex NO.
\item Hinge loss: $\varphi(u) = \max\{0,1+u\}$: Lipschitz YES, Smooth NO, strongly-convex NO.
\item Logistic loss: $\varphi(u) = \log_2(1+\exp(u))$: Lipschitz YES, Smooth YES, strongly-convex NO.
\item Least-squares loss: $\ell(u,y) = (u-y)^2$: Lipschitz NO, Smooth YES, strongly-convex YES.
\end{itemize}

Note that as far as the empirical risk minimization goes, what we care about are the (almost sure) properties of the function $x\in\mathcal{X}\rightarrow R(x) := \frac{1}{m} \sum_{i=1}^m R_i(x) = \frac{1}{m} \sum_{i=1}^m \ell(x^T\Phi(W_i),Y_i)$. In particular, if $\mathcal{X}$ is bounded, we can restrict the definitions above to hold on a bounded set. For instance, we could define $L$-Lipschitz as the property: $| f(x) - f(y) | \le L \| x - y \|_2$ for any $x,y\in\mathbb{R}^n$ such that $\|x\|_2,\|y\|_2\le c$ ($\|\nabla f(x)\|_2 \le L$ for any $x\in\mathbb{R}^n$ such that $\|x\|_2\le c$). Assuming boundedness provides extra flexibility, allowing one to directly use these properties in setting where otherwise they would not apply (take the exponential loss, for instance).
%Otherwise, approximation techniques need to be used to deal with non-boundedness.

Even if we assume boundedness of $\mathcal{X}$, assuming that the empirical risk $R$ is strongly convex is typically a strong assumption. In fact, note that \emph{for any} $x\in\mathcal{X}$ (that is, regardless of the properties of $\mathcal{X}$) the Hessian
\begin{align*}
	\nabla^2R(x) = \frac{1}{m} \sum_{i=1}^m \frac{\partial^2}{\partial z^2} \ell(x^T\Phi(W_i),Y_i) 
	\, \Phi(W_i)\Phi(W_i)^T\in\mathbb{R}^{n\times n}
\end{align*}
is not invertible if $m < n$, being a sum of $m<n$ rank-1 matrices. So in this case $x\rightarrow\nabla^2R(x)$ can not be strongly convex. In applications where $m\ge n$, the empirical covariance can be invertible, but it is typically the case that the strong convexity parameter $\alpha$ is very small, of order $1/m$, effectively $\alpha \approx 0$ (or close to machine precision) for applications involving large datasets. For this reason, in this reading group we will only focus on results about Lipschitz and smooth functions, which are more natural assumptions for the empirical risk $R$, as we now show.

First, note that if we assume that $\ell$ is $L_{\textrm{loss}}$-Lipschitz in the first coordinate, namely, $z\rightarrow \ell(z,y)$ is $L_\textrm{loss}$-Lipschitz for any $y\in\mathcal{Y}$, and that the feature map $\Phi$ is bounded in $\ell_2$, namely, $\|\Phi\|_2\le G$, then $R$ is $GL_\textrm{loss}$-Lipschitz:
$$
	| R(x) - R(y) | 
	\le \frac{1}{m} \sum_{i=1}^m | \ell(x^T\Phi(W_i),Y_i)-\ell(y^T\Phi(W_i),Y_i) |
	\le \frac{L_\textrm{loss}}{m} \sum_{i=1}^m \| (x-y)^T\Phi(W_i)) \|_2
	\le GL_\textrm{loss} \|x-y\|_2,
$$
where for the last inequality we used Cauchy-Schwarz. Second, note that if we assume that $\ell$ is $\beta_{\textrm{loss}}$-Lipschitz in the first coordinate, namely, $z\rightarrow \ell(z,y)$ is $\beta_\textrm{loss}$-Lipschitz for any $y\in\mathcal{Y}$, and that $\|\Phi\|_2\le G$, then $R$ is $G^2\beta_\textrm{loss} $-smooth:
\begin{align*}
	\| \nabla R(x) - \nabla R(y) \|_2 
	&\le \frac{1}{m} \sum_{i=1}^m \bigg\| \bigg(\frac{\partial}{\partial z} \ell(x^T\Phi(W_i),Y_i)-\frac{\partial}{\partial z}\ell(y^T\Phi(W_i),Y_i)\bigg) \Phi(W_i) \bigg\|_2\\
	&\le \frac{1}{m} \sum_{i=1}^m \beta_{\textrm{loss}} 
	| (x-y)^T\Phi(W_i) |
	\| \Phi(W_i) \|_2
	\le \beta_{\textrm{loss}} G^2 \|x-y\|_2.
\end{align*}

While this is a reading group on optimization, before we embark on the adventure of minimizing the empirical risk and bound the \emph{OPTIMIZATION} term in \eqref{bound:stats-opt}, we should try to develop some basic understanding of what to expect from the behavior of the \emph{STATISTICS} term as well. This understanding will give us insights on the type of precision that we should strive for to control the \emph{OPTIMIZATION} term. To this end, we now give a brief detour on statistical learning theory and derive an explicit bound in the case when the loss function $\ell$ is Lipschitz (no assumption on convexity is made for this result), the feature map is bounded, and also the parameter space $\mathcal{X}$ is bounded. While in this reading group we do not plan to prove other results such as the one that we present in the next section, nevertheless the proof is instructive and relies on classical machinery of concentration inequalities and Rademacher complexity.

\subsection{Statistical learning theory}
\label{sec:Statistical learning theory}
Statistical learning theory focuses on understanding how the estimation error $r(X^\star) - r(x^\star)$ converges to zero as a function of the sample size $m$, the loss function $\ell$, the parameter space $\mathcal{X}$, and possibly some properties of the distribution of $\Phi(W)$ and $Y$ (recall that the distribution themselves are not known). We will prove the following result, following the arguments as presented in \cite{bach}.

\begin{proposition}
\label{prop:Lip-stats}
If $z\rightarrow \ell(z,y)$ is $L_\textrm{loss}$-Lipschitz for any $y\in\mathcal{Y}$, $\|\Phi(W)\|_2\le G$ a.s., and $\sup_{x\in\mathcal{X}}\|x\|_2 \le B$, then with probability at least $1-\delta$ we have
$$
	\textrm{STATISTICS} = \sup_{x\in\mathcal{X}} ( r(x) - R(x) ) + \sup_{x\in\mathcal{X}} ( R(x) - r(x) )
	\le 2\frac{\ell_0+GL_\textrm{loss}B}{\sqrt{m}}\left(4 + \sqrt{2\log \frac{1}{\delta}} \right),
$$
where $\ell_0:=\sup_{y\in\mathcal{Y}} | \ell(0,y) |$.
\end{proposition}

The quality of the bound in Proposition \ref{prop:Lip-stats} is typical in statistical learning theory. In particular, it is typically the case that the \emph{slow} rate $O(1/\sqrt{m})$ can not be improved, unless other assumptions are taken into consideration that allow to prove a \emph{fast} rate $O(1/m)$. These upper bounds suggest\footnote{Caveats are in order: after all the result in \eqref{bound:stats-opt} is only an upper-bound, and it is often the case that in practice one does not know the constants sitting in front of the error bounds that one can prove for both the \emph{OPTIMIZATION} term and the \emph{STATISTICS} term, i.e., the constants $G,L,R$ in our case.} that we only need to approximate the \emph{OPTIMIZATION} term up to precision $O(1/\sqrt{m})$ (resp. $O(1/m)$).
%In particular, the general message is that it is only useful to optimize to precision which is of the same order of the estimation error (assuming that we know $G,L,R$).\\

The next part of this section is devoted to proving Proposition \ref{prop:Lip-stats}. To this end, we need to bound suprema of a random processes. Henceforth, let us consider the following quantity (analogously, we can look at the symmetric quantity):
\begin{align}
	h(Z_1,\ldots,Z_n) := \sup_{x\in\mathcal{X}} ( r(x) - R(x) )
	= \sup_{x\in\mathcal{X}} \bigg( r(x)- \frac{1}{m} \sum_{i=1}^m R_i(x) \bigg),
	\label{def:h}
\end{align}
where we use the notation $Z_i := (W_i,Y_i)$. A classical way to bound this quantity is by using concentration inequalities, which are tools in probability theory to bound the deviation of a function of many i.i.d.\ random variables to its mean. One of the classical concentration inequalities is the Bounded Difference, of which we now state a simple version.

\begin{proposition}[Bounded Difference Inequality]
\label{prop:BDI}
Let $Z_1,\ldots,Z_m\in\mathcal{Z}$ be $m$ i.i.d.\ random variables, and let $h:\mathcal{Z}^m\rightarrow\mathbb{R}$ be a function satisfying, for every $k\in\{1,\ldots,m\}$ and for all $z_1,\ldots,z_m,z_k'\in\mathcal{Z}$:
$$
	| h(z_1,\ldots,z_{k-1},z_k,z_{k+1},\ldots,z_m) - h(z_1,\ldots,z_{k-1},z_k',z_{k+1},\ldots,z_m) | \le c.
$$
Then,
$$
	\mathbf{P}(h(Z_1,\ldots,Z_m) \ge \mathbf{E}h(Z_1,\ldots,Z_m) + t ) \le \exp\bigg(-\frac{2t^2}{m c^2}\bigg),
$$
or, equivalently, with probability at least $1-\delta$ we have
$$
	h(Z_1,\ldots,Z_m) \le \mathbf{E}h(Z_1,\ldots,Z_m) + c\sqrt{\frac{m}{2}\log\frac{1}{\delta}}.
$$
\end{proposition}
To apply the Bounded Difference inequality to the function $h$ defined in \eqref{def:h}, we need to find $c$ that satisfy the assumption of Proposition \ref{prop:BDI}. The next proposition shows how to do this under the assumption of Proposition \ref{prop:Lip-stats}.
\begin{proposition}
\label{prop:c}
Let $h$ be the function defined in \eqref{def:h} and consider the same assumption of Proposition \ref{prop:Lip-stats}. Then, $c\in\mathbb{R}$ satisfying the requirement of the Bounded Difference inequality is given by
$$
	c = \frac{2}{m} \bigg(\sup_{y\in\mathcal{Y}} | \ell(0,y) | + GL_\textrm{loss}B \bigg).
$$
\end{proposition}

\begin{proof}
Fix $k\in\{1,\ldots,m\}$ and let $z=(z_1,\ldots,z_{k-1},z_k,z_{k+1},\ldots,z_m)$ and $z'=(z_1,\ldots,z_{k-1},z'_k,z_{k+1},\ldots,z_m)$. Then, 
\begin{align*}
	| h(z) - h(z') |
	= \bigg| \sup_{x\in\mathcal{X}} \bigg( r(x)- \frac{1}{m} \sum_{i=1}^m R^z_i(x) \bigg)
	- \sup_{x\in\mathcal{X}} \bigg( r(x)- \frac{1}{m} \sum_{i=1}^m R^{z'}_i(x) \bigg) \bigg|,
\end{align*}
where $R^z_i(x) := \ell(x^T\Phi(w_i),y_i)$. If $h(z) - h(z') \ge 0$ and we let $\tilde x\in\mathcal{X}$ be the maximizer of $\sup_{x\in\mathcal{X}} ( r(x)- \frac{1}{m} \sum_{i=1}^m R^z_i(x) )$ (note that the supremum is attained by the Extreme Value Theorem), we have
\begin{align*}
	h(z) - h(z') 
	&= \bigg( g(\tilde x)- \frac{1}{m} \sum_{i=1}^m R^z_i(\tilde x) \bigg) - \sup_{x\in\mathcal{X}} \bigg( r(x)- \frac{1}{m} \sum_{i=1}^m R^{z'}_i(x) \bigg)\\
	&\le \bigg( g(\tilde x)- \frac{1}{m} \sum_{i=1}^m R^z_i(\tilde x) \bigg) - \bigg( g(\tilde x)- \frac{1}{m} \sum_{i=1}^m R^{z'}_i(\tilde x) \bigg)\\
	&= \frac{1}{m} ( R^z_k(\tilde x) - R^{z'}_k(\tilde x) )
	\le \frac{2}{m} \sup_{x\in\mathcal{X},z\in\mathcal{Z}} | R^z_k(x) |.
\end{align*}
Proceeding analogously in the case $h(z) - h(z') \le 0$, we finally find
$$
	| h(z) - h(z') | \le \frac{2}{m} \sup_{x\in\mathcal{X},z\in\mathcal{Z}} | R^z_k(x) |
	= \frac{2}{m} \sup_{x\in\mathcal{X},w\in\mathcal{W},y\in\mathcal{Y}} | \ell(x^T\Phi(w),y) |.
$$
Using, that $z\rightarrow \ell(z,y)$ is $L_\textrm{loss}$-Lipschitz for any $y\in\mathcal{Y}$ and Cauchy-Schwarz we get
$$
	|\ell(x^T\Phi(w),y)| \le | \ell(0,y) | + |\ell(x^T\Phi(w),y) - \ell(0,y)|
	\le | \ell(0,y) | + L_\textrm{loss}\|x^T\Phi(w)\|_2
	\le | \ell(0,y) | + L_\textrm{loss}\|x\|_2\|\Phi(w)\|_2,
$$
which, upon using $\|\Phi(W)\|_2\le G$ a.s. and $\sup_{x\in\mathcal{X}}\|x\|_2 \le B$ yields
$$
	|\ell(x^T\Phi(w),y)|
	\le | \ell(0,y) | + GL_\textrm{loss}B,
$$
so that
$$
	| h(z) - h(z') | \le \frac{2}{m} \bigg(\sup_{y\in\mathcal{Y}} | \ell(0,y) | + GL_\textrm{loss}B \bigg).
$$
\end{proof}

The bounded-difference inequality tells us that in order to control $h(Z_1,\ldots,Z_m)$ we need to control the expectation $\mathbf{E}[h(Z_1,\ldots,Z_m)]$. The following is a classical way of doing so, based on the Rademacher complexity.

\begin{proposition}
\label{prop:radamacher}
We have
$$
	\mathbf{E}\sup_{x\in\mathcal{X}} \bigg( r(x) - \frac{1}{m}\sum_{i=1}^m R_i(x) \bigg)
	\le 
	2 \mathcal{R},
$$
where $\mathcal{R}$ is the Rademacher complexity defined as
$$
	\mathcal{R}:=\mathbf{E} \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i R_i(x) ,
$$
with $\varepsilon_1,\ldots,\varepsilon_n$ i.i.d.\ Rademacher random variables uniform in $\{-1,1\}$, independent of $Z_1,\ldots,Z_n$. 
\end{proposition}

\begin{proof}
The proof uses the standard machinery of symmetrization. Let $\mathcal{D}':=\{Z_1',\ldots,Z_m'\}$ be an independent copy of the data $\mathcal{D}:=\{Z_1,\ldots,Z_m\}$. Using that $r(x)=\mathbf{E}R_i'(x)=\mathbf{E}[R_i'(x)|\mathcal{D}]$, we have
\begin{align*}
	\mathbf{E}\sup_{x\in\mathcal{X}} \bigg( r(x) - \frac{1}{m}\sum_{i=1}^m R_i(x) \bigg)
	&=
	\mathbf{E}\sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m \mathbf{E} [ R_i'(x) - R_i(x) ) | \mathcal{D}]\\
	&\le
	\mathbf{E}\mathbf{E}\bigg[\sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m ( R_i'(x) - R_i(x)) \bigg| \mathcal{D}\bigg]\\
	&=
	\mathbf{E}\sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m ( R_i'(x) - R_i(x) )\\
	&=
	\mathbf{E}\sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m \varepsilon_i (R_i'(x) - R_i(x)) \quad \text{by symmetrization}\\
	&=
	2\mathbf{E}\sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m \varepsilon_i R_i(x).
\end{align*}
\end{proof}

We are left with bounding the Rademacher complexity.
\begin{proposition}
\label{prop:boundonradamacher}
Under the assumption of Proposition \ref{prop:Lip-stats}, we have
$$
	\mathcal{R}
	\le
	\frac{2GL_\textrm{loss}B}{\sqrt{m}}.
$$
\end{proposition}

\begin{proof}
Note that there are two independent sources of randomness in the definition of $\mathcal{R}$. One comes from the Rademacher random variables $\varepsilon_1,\ldots,\varepsilon_m$, the other comes from the data $\mathcal{D}:=(Z_1,\dots,Z_m)$ itself. By the tower property we can isolate the effect of the Rademacher random variables and work conditionally on $\mathcal{D}$. That is,
\begin{align*}
	\mathcal{R} 
	&= \mathbf{E} \mathbf{E} \bigg[ \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i R_i(x) \bigg| \mathcal{D}\bigg].
\end{align*}
If for each $i\in\{1,\ldots,m\}$ we define the functions $\varphi_i(u) := \ell(u,Y_i) - \ell(0,Y_i)$, we have
\begin{align*}
	\mathbf{E} \bigg[ \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i R_i(x) \bigg| \mathcal{D} \bigg]
	&= \mathbf{E} \bigg[ \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i [R_i(0) + R_i(x)-R_i(0)] \bigg| \mathcal{D} \bigg]\\
	&\le \mathbf{E} \bigg[ \frac{1}{m}\sum_{i=1}^m\varepsilon_i R_i(0) \bigg| \mathcal{D} \bigg]
	+ \mathbf{E} \bigg[ \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i [\ell(x^T\Phi(W_i),Y_i)-\ell(0,Y_i)] \bigg| \mathcal{D} \bigg]\\
	&= \frac{1}{m}\sum_{i=1}^m \mathbf{E} [\varepsilon_i | \mathcal{D}] R_i(0) + 
	\mathbf{E} \bigg[ \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i \varphi_i(x^T\Phi(W_i)) \bigg| \mathcal{D} \bigg]\\
	&= \mathbf{E} \bigg[ \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i \varphi_i(x^T\Phi(W_i)) \bigg| \mathcal{D} \bigg],
\end{align*}
where for the last equality we used that by the independence between $\varepsilon_1,\ldots,\varepsilon_m$ and $\mathcal{D}$ we have $\mathbf{E} [\varepsilon_i | \mathcal{D}]=\mathbf{E} \varepsilon_i = 0$. As the function $z\rightarrow \ell(z,y)$ is $L_\textrm{loss}$-Lipschitz for any $y\in\mathcal{Y}$, also each function $\varphi_i$ is $L_\textrm{loss}$-Lipschitz. By the contraction property of Rademacher complexities (indeed, one of the reason why this notion of complexity is convenient to use), we have
$$
	\mathbf{E} \bigg[ \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i \varphi_i(x^T\Phi(W_i)) \bigg| \mathcal{D} \bigg]
	\le
	2L_\textrm{loss}\mathbf{E} \bigg[ \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i x^T\Phi(W_i) \bigg| \mathcal{D} \bigg].
$$
Hence, using that $\sup_{x\in\mathcal{X}}\|x\|_2 \le B$
\begin{align*}
	\mathcal{R} 
	&\le 2L_\textrm{loss}\mathbf{E} \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i x^T\Phi(W_i)\\
	&\le 2L_\textrm{loss}\mathbf{E} \sup_{x\in\mathcal{X}} \| x \|_2\bigg\|\frac{1}{m}\sum_{i=1}^m\varepsilon_i \Phi(W_i)\bigg\|_2 \quad \text{by Cauchy-Schwarz}\\
	&\le 2L_\textrm{loss}B\mathbf{E} \sqrt{\bigg\|\frac{1}{m}\sum_{i=1}^m\varepsilon_i \Phi(W_i)\bigg\|_2^2}
	\le 2L_\textrm{loss}B \sqrt{ \mathbf{E} \bigg\|\frac{1}{m}\sum_{i=1}^m\varepsilon_i \Phi(W_i)\bigg\|_2^2} \quad \text{by Jensen's inequality}\\
	&= \frac{2L_\textrm{loss}B}{m} \sqrt{ \mathbf{E} \sum_{j=1}^n \bigg(\sum_{i=1}^m \varepsilon_i \Phi(W_i)_j\bigg)^2 } 
	= \frac{2L_\textrm{loss}B}{m} \sqrt{ \mathbf{E} \sum_{j=1}^n \sum_{i=1}^m (\varepsilon_i \Phi(W_i)_j)^2 } \quad \text{by the independence of the $\varepsilon$'s}\\
	&= \frac{2L_\textrm{loss}B}{m} \sqrt{ \mathbf{E}  \sum_{i=1}^m \varepsilon_i^2 \| \Phi(W_i)\|^2_2 }
	\le \frac{2GL_\textrm{loss}B}{\sqrt{m}} \quad \text{as $\|\Phi(W)\|_2\le G$ a.s.}\\
\end{align*}
\end{proof}

The proof of Proposition \ref{prop:Lip-stats} follows by putting everything together, namely, Proposition \ref{prop:BDI}, Proposition \ref{prop:c}, Proposition \ref{prop:radamacher}, and Proposition \ref{prop:boundonradamacher}, noting that the bounds in high probability that we have obtained hold for \emph{both} $\sup_{x\in\mathcal{X}} ( r(x) - R(x) )$ and $\sup_{x\in\mathcal{X}} ( R(x) - r(x) )$. Here are the details.

\begin{proof}[Proof of Proposition \ref{prop:Lip-stats}]
Using, respectively, Proposition \ref{prop:BDI} and Proposition \ref{prop:radamacher}, we have that with probability at least $1-\delta$ the following holds:
$$
	\sup_{x\in\mathcal{X}} ( r(x) - R(x) ) \le 
	\mathbf{E}\bigg[\sup_{x\in\mathcal{X}} ( r(x) - R(x) )\bigg] + c\sqrt{\frac{m}{2}\log\frac{1}{\delta}}
	\le 2 \mathcal{R} + c\sqrt{\frac{m}{2}\log\frac{1}{\delta}},
$$
where $c = \frac{2}{m} (\ell_0 + GL_\textrm{loss}B )$ by Proposition \ref{prop:c}. Plugging in the bound for $\mathcal{R}$ in Proposition \ref{prop:boundonradamacher}, we get
$$
	\mathbb{P}\bigg(\sup_{x\in\mathcal{X}} ( r(x) - R(x) ) \le c\bigg) \ge 1-\delta,
$$
where $c:=2(\ell_0+GL_\textrm{loss}B)(4 + \sqrt{2\log(1/\delta)})/\sqrt{m}$. As the bounds we have derived holds also for $\sup_{x\in\mathcal{X}} ( R(x) - r(x) )$, we have
$$
	\mathbb{P}\bigg(\sup_{x\in\mathcal{X}} ( r(x) - R(x) ) + \sup_{x\in\mathcal{X}} ( R(x) - r(x) ) \le 2c \bigg)
	\ge \mathbb{P}\bigg(\bigg\{\sup_{x\in\mathcal{X}} ( r(x) - R(x) ) \le c \bigg\} \bigcap \bigg\{\sup_{x\in\mathcal{X}} ( R(x) - r(x) ) \le c \bigg\} \bigg) \ge 1-\delta.
$$
\end{proof}

\subsection{Plan for the rest of the term}

Given the analysis of the previous section, we can now describe the plan for the reading group this term.
While we motivated our interest in optimization techniques by looking at the classical setting of supervised machine learning and empirical risk minimization, only in Week 8 (hopefully!) we will see how to develop specialized algorithms to minimize $R$ (that is, algorithms that can take advantage of the specific structure of $R$). In fact, most of our time will be devoted to reviewing classical algorithms to optimize a \emph{general} convex function $f$ over a convex set $\mathcal{X}$, namely, to solve:
\begin{align*}
	\begin{aligned}
		\text{minimize }\quad   & f(x)\\
		\text{subject to }\quad & x\in\mathcal{X}.
	\end{aligned}
\end{align*}


Below is the current plan of what we aim to cover, along with references to the corresponding literature.\\

\fbox{\begin{minipage}{46em}
\textbf{IMPORTANT NOTE FOR SPEAKERS.}\\The results that are quoted in bold are the ones we will be focusing on. When presenting your section and typing up notes, focus only on covering these results, along with introducing the quantities/proofs the are needed. You might comment on the other results if you have time, or just to give an overview of the bigger picture. In other words, instead of trying to cover as much as possible content-wise, try to cover as little as possible with as much explanation as possible (pictures, intuition, etc.).\\
As we move along in the reading group, the plan below might change. Please make sure that you have the latest version of these lecture notes before you prepare your section.
\end{minipage}}


\subsubsection*{Week 2: Convexity. Black-box model. Projected gradient descent methods.}
Readings: Parts of Chapter 1 and Chapter 3 in \cite{bubeck}. Details are below.\\

We introduce convexity, and some of its basic properties, such as the existence of subgradients (\textbf{Proposition 1.1 in \cite{bubeck}}) and the first order optimality condition (\textbf{Proposition 1.3 in \cite{bubeck}}).

We introduce the black-box model of computation, where we assume that the constraint set $\mathcal{X}\subseteq\mathbb{R}^n$ is known and the objective function $f$ is unknown but can be accessed through queries to first order oracles: given $x\in\mathcal{X}$, a first order oracle yields back a subgradient of $f$ at $x$.

We study how the different assumptions of $L$-Lipschitz, $\beta$-Smoothness, and $\alpha$-Strong convexity, lead to different convergence rates for projected subgradient descent methods. Upon different conditions, different projected subgradient descent methods achieve the following rates:
\begin{center}
 \begin{tabular}{|c | c | c|}
 \hline
 & $L$-Lipschitz & $\beta$-smooth\\
 \hline
 Convex & $O(BL/\sqrt{t})$ & $O(B^2\beta/t)$\\
 \hline
 $\alpha$-strongly convex & $O(L^2/(\alpha t))$ & $O(B^2e^{-t\alpha/\beta})$\\
 \hline
\end{tabular}
\end{center}
where $B$ is the radius of the Euclidean ball that contains $\mathcal{X}$, namely, $\sup_{x\in\mathcal{X}}\|x\|_2 \le B$.
As we discuss earlier, for applications in machine learning the assumption of $\alpha$-strong convexity is typically too strong. This is why we only prove the rates for the convex case, both for $L$-Lipschitz and for $\beta$-Smooth. For completeness, here is where to find all the results mentioned above:
\begin{itemize}
	\item Convex \& $L$-Lipschitz: $O(BL/\sqrt{t})$ (\textbf{Theorem 3.2 in \cite{bubeck}}).
	\item Convex \& $\beta$-Smoothness: $O(B^2\beta/t)$  (\textbf{Theorem 3.7 and in \cite{bubeck}}).
	\item $\alpha$-Strong convexity \& $L$-Lipschitz: $O(L^2/(\alpha t))$ (Theorem 3.9 and in \cite{bubeck}).
	\item $\alpha$-Strong convexity \& $\beta$-Smoothness: $O(B^2\exp(-t\alpha/\beta))$ (Theorem 3.10 and in \cite{bubeck}).
\end{itemize}	


These results are sometimes called \emph{dimension-free}, since as presented the rates do not depend explicitly on the ambient dimension $n$. However, the dependence on the dimension enters implicit in the constants: note that $B$ is the radius of the constraint set $\mathcal{X}\subseteq \mathbb{R}^n$, and that the parameters $L$, $\alpha$, and $\beta$, are defined in terms of the Euclidean norm in $\mathbb{R}^n$. The terminology dimension-free is used in \cite{bubeck} to differentiate the rates achieved by subgradient descent methods from the rates obtained by ellipsoid methods (Chapter 2 in \cite{bubeck}), which we will not cover in this reading group.
	
We can also phrase these results in terms of \emph{oracle complexity}, namely, the number of queries to the oracle that are \emph{sufficient} to find an $\varepsilon$-approximate minima of a convex function:
\begin{center}
 \begin{tabular}{|c | c | c|}
 \hline
 & $L$-Lipschitz & $\beta$-smooth\\
 \hline
 Convex & $O(B^2L^2/\varepsilon^2)$ & $O(B^2\beta/\varepsilon)$\\
 \hline
 $\alpha$-strongly convex & $O(L^2/(\alpha \varepsilon))$ & $O((\beta/\alpha)\log{(B^2/\varepsilon)})$\\
 \hline
\end{tabular}
\end{center}

	
\begin{remark}\label{rem:optimization}
To conclude, let us go back to our motivating example of minimizing the empirical risk function $R(x) = \frac{1}{m}\sum_{i=1}^m \ell(x^T\Phi(W_i),Y_i)$. Under the assumptions of Proposition \ref{prop:Lip-stats}, $R$ is $GL_\textrm{loss}$-Lipschitz (see Section \ref{sec:introduction}). Assuming that the loss function $\ell$ is convex, the subgradient descent method then yields:
$$
	\textrm{OPTIMIZATION} := R(\hat X_t) - R(X^\star) \le \frac{GL_\textrm{loss}B}{\sqrt{t}}.
$$
Here the constants are explicit, and they match the ones in Proposition \ref{prop:Lip-stats} for the \textrm{STATISTICS} term. In particular, following the rationale of optimizing the \emph{OPTIMIZATION} term in \eqref{bound:stats-opt} up to the accuracy given by the \emph{STATISTICS} term in \eqref{bound:stats-opt}, one finds that it suffices to run the algorithm for $t\sim m$ number of steps.
\end{remark}

\subsubsection*{Week 3: Lower bounds for oracle complexity.}
Readings: Parts of Chapter 3 in \cite{bubeck}. Details are below.\\

We see that within the first order oracle model one can prove lower-bounds for the amount of calls to the oracle that are \emph{needed} to achieve a certain accuracy $\varepsilon$.
In the non-smooth case we show that the rates achieved by subgradient descent methods, i.e., $O(BL/\sqrt{t})$ for convex functions and $O(L^2/(\alpha t))$ for $\alpha$-strongly convex functions, can not be improved (\textbf{Theorem 3.13 in \cite{bubeck}; we will only cover the proof of the first statement for convex $L$-Lipschitz functions}).
On the other hand, in the smooth case we prove oracle-complexity lower bounds that are better than the ones achieved by subgradient methods, namely, $O(D^2\beta/t^2)$ for smooth functions (\textbf{Theorem 3.14 in \cite{bubeck}}) and $O(D^2\exp(-t\sqrt{\alpha/\beta}))$ for smooth and strongly convex functions (Theorem 3.15 in \cite{bubeck}), where $D$ is the diameter of $\mathcal{X}$, i.e., $D:=\max_{x,y\in\mathcal{X}}\|x-y\|_2$. To recap, the optimal rates are:
\begin{center}
 \begin{tabular}{|c | c | c|}
 \hline
 & $L$-Lipschitz & $\beta$-smooth\\
 \hline
 Convex & $O(BL/\sqrt{t})$ & $O(D^2\beta/t^2)$\\
 \hline
 $\alpha$-strongly convex & $O(L^2/(\alpha t))$ & $O(D^2e^{-t\sqrt{\alpha/\beta}})$\\
 \hline
\end{tabular}
\end{center}

\subsubsection*{Week 4: Application: Boosting.}
Reading: Parts of Part II, Section 1.4 and Part II, Section 2.2 in \cite{rigollet}. Details are below.\\

We apply the projected subgradient descent algorithm to solve an important example in machine learning: Boosting. For a given loss function $\varphi$ (recall, $\ell(z,y) = \varphi(-zy)$ in classification), $\ell_1$-boosting can be written as the problem:
\begin{align*}
	\begin{aligned}
		\text{minimize }\quad   & r(x) = \mathbf{E} \varphi(-Yx^T\Phi(W))\\
		\text{subject to }\quad & x\in \Delta_m,
	\end{aligned}
\end{align*}
where $\Delta_m := \{x\in [0,1]^m : \sum_{k=1}^m x_k = 1\}$ is the $m$-dimensional probability simplex. Here the feature map $\Phi : \mathcal{W} \rightarrow \{-1,1\}^n$ encodes the prediction of the $n$ base classifiers on the given data point it is applied to, and $x^T\Phi$ is a linear combination of these classifiers. 
Following the error decomposition in Proposition \ref{prop:erm2}, we show (\textbf{Part II, Section 1.4 in \cite{rigollet}}) that in this case the \emph{STATISTICS} term is $O(\log(n)/\sqrt{m})$, which only grows logarithmically with the number of base classifiers $n$. This is a nice feature in applications where $n$ is very large. Hence, one would hope to be able to design an algorithmic procedure that only uses $\log(n)$ calls to the oracle in order to match the accuracy of the \emph{STATISTICAL} term when finding a solution to the empirical risk minimization problem:
\begin{align*}
	\begin{aligned}
		\text{minimize }\quad   & R(x) = \frac{1}{m} \sum_{i=1}^m \varphi(-Y_ix^T\Phi(W_i))\\
		\text{subject to }\quad & x\in \Delta_m.
	\end{aligned}
\end{align*}
We show that if $\phi$ is $L$-Lipschitz, then the empirical risk minimizer $R$ is $L\sqrt{n}$-Lipschitz, so that projected subgradient descent yields a rate $O(L\sqrt{n/t})$ (\textbf{Part II, Section 2.3.3 in \cite{rigollet}}). Imposing the condition $L\sqrt{n/t} \lesssim \log(n)/\sqrt{m}$, we find that $t\gtrsim L^2 nm/\log(n)^2$, which does \emph{not} scale logarithmically with $n$ as we hoped for. Boosting is one example where one would like to apply subgradient descent methods on a non-Euclidean space, in particular a $\ell_1$ space, but these methods are not suited for this type of problems as they are really designed for Euclidean geometry. Note, in fact, that all the definitions we gave for $L$-Lipschitz, $\beta$-smoothness, $\alpha$-strong convexity are expressed in terms of the Euclidean norm $\|\,\cdot\,\|_2$. This motivates the design of a new class of algorithms that can adapt to the geometry of the problem at hand, as we will see next.
%Since, as we saw in Section \ref{sec:Statistical learning theory}, the statistical term is often $O(1/\sqrt{m})$ or $O(1/m)$ in some favourable scenarios, to be generous we would like the optimization term to be bounded by $O(1/m)$, which would yield $t \gtrsim L^2 n m^2$.

\subsubsection*{Week 5: Non-Euclidean setting: mirror descent.}
Readings: Section 4.1, 4.2, and 4.3 in \cite{bubeck}. Details are below.\\

%As we saw last time, Frank-Wolfe is an algorithm that can adapt to smoothness in any norm, not just the Euclidean $\ell_2$ norm. What about an algorithm that can adapt to other properties, such as $L$-Lipschitz or the radius $R$ of $\mathcal{X}$, in any norm? 
We introduce the mirror descent algorithm and show that this algorithm adapts also to non-Euclidean geometries. We say that $f$ is $L$-Lipschitz in some norm $\|\,\cdot\,\|$ if $|f(x) - f(y)| \le L \| x - y \|$ for any $x,y\in\mathbb{R}^n$. We show that for such convex functions mirror descent yields rates that scale like $O(LB/\sqrt{t})$ (\textbf{Theorem 4.2 in \cite{bubeck}}). This allows mirror descent to achieve a rate $O(\sqrt{\log n/t})$ in the example of boosting, where one wants to minimize a function with subgradient bounded in the $\ell_\infty$-norm over the probability simplex (\textbf{Section 4.3 in \cite{bubeck}}), in contrast to the rate $O(\sqrt{n/t})$ achieved by subgradient descent, as shown last time. We also show that in the case of the Euclidean norm, the algorithm is equivalent to projected subgradient descent.

\subsubsection*{Week 6: Acceleration by coupling gradient descent and mirror descent.}
Readings: \cite{linearcoupling}.\\

We show that mirror descent can be coupled with gradient descent to yield an \emph{accelerated} algorithm that can achieve the lower bound for smooth functions that we proved in Week 3. The acceleration technique is linear coupling, from the paper \cite{linearcoupling}.

%that a modification of the original subgradient descent technique yields optimal convergence rates in terms of oracle complexity. In particular, we show that this method achieves the rate $O(D^2\beta/t^2)$ for the smooth case (\textbf{Theorem 3.19 in \cite{bubeck}}). The method achieves the rate $O((\alpha+\beta)D^2\exp(-t\sqrt{\alpha/\beta}))$ for the smooth and strongly convex case (Theorem 3.18 in \cite{bubeck}).

\subsubsection*{Week 7: Non-Euclidean setting: Frank-Wolfe.}
Readings: Section 3.3 in \cite{bubeck}. Details are below.\\

In many applications, the computational bottleneck in gradient descent methods is given by the projection step on the constraint set $\mathcal{X}$. To address this issue, we study the Frank-Wolfe algorithm (a.k.a.\ conditional gradient descent), which replaces the projection step with a linear optimization over $\mathcal{X}$, which in some cases can be a much simpler problem. For smooth objective functions, the Frank-Wolfe algorithm achieves rate $O(B^2\beta/t)$ (\textbf{Theorem 3.8 in \cite{bubeck}}). While this rare is slow and it does not match the oracle complexity lower bound $O(D^2\beta/t^2)$, the saving in the computational complexity makes this algorithm convenient in some applications. Other advantages of this algorithm over gradient descent methods is that this algorithm can apply to smoothness in any norm ($f$ is $\beta$-smooth in some norm $\|\,\cdot\,\|$ if $\| \nabla f(x) - \nabla f(y) \|_* \le \beta \| x - y \| $ for any $x,y\in\mathbb{R}^n$, where the dual norm $\|\,\cdot\,\|_*$ is defined as $\|g\|_* = \sup_{x\in\mathbb{R}^n:\|x\|\le 1} g^Tx$), and that the algorithm computes sparse iterates. We will discuss a concrete application where these benefits are substantials, the least square regression with structured sparsity (\textbf{in Section 3.3 in \cite{bubeck}}).

\subsubsection*{Week 8: Stochastic oracle model.}
Readings: Section 6.1, 6.2, and 6.3 in \cite{bubeck}.\\

The first order oracle model that we have investigated so far had allowed us to produce a complete theory of convex optimization, in the sense that for various classes of convex functions we can design algorithms with an \emph{oracle complexity} that matches the lower bounds. However, the black-box model does not tell us anything about the \emph{computational complexity}, namely, the number of elementary computations that an algorithm needs to do to solve the problem (indeed, to address this question we should ``open'' the black-box). Going back to the original problem in machine learning that we set to solve, as described in Section \ref{sec:introduction}, we note that the first order oracle model is too general for our needs, and that there might be computational savings to be gained in working with a model of computation that is more fined-tuned to our problem. This consideration motivates us to consider the \emph{stochastic} first order oracle model, where for any point $x\in\mathcal{X}$ the oracle gives back an unbiased estimator of the gradient at $x$. Within this framework, two  approaches have attracted a lot of attention lately, due to the computational savings they yield.

\underline{Multiple passes over the data.}
Within the setting of empirical risk minimization, one notices that we know the \emph{global} structure of the function we want to minimize, namely, $R=\frac{1}{m}\sum_{i=1}^m R_i$. Hence, for instance, given $x\in\mathcal{X}$ we can have access to $\nabla R_i(x)$ for a specific $i\in\{1,\ldots,m\}$, which is something that is not allowed in the oracle model previously discussed where only access to $\nabla R(x)$ is granted. Note that computing $\nabla R(x)$ costs $O(m)$ operations as $R$ is the sum of $m$ terms, while computing $\nabla R_i(x)$ costs $O(1)$.

\underline{Single pass over the data.}
The empirical risk minimization approach described in Section \ref{sec:introduction} has allowed us to break the original problem we care about, namely, problem \eqref{def:mainproblem}, into a \emph{STATISTICAL} component and an \emph{OPTIMIZATION} component, see Proposition \ref{prop:erm2}. We show that within the stochastic oracle model there is a way to directly address problem \eqref{def:mainproblem} and yield a bound for $r(\hat X_t) - r(x^\star)$, \emph{combining} both statistics and optimization.
%This can be done allowing access to unbiased estimates of the gradient of $g$, not $R$. 
Recall that we do not know the function $r$ as we assume that we do not know the distribution of the random variables $W$ and $Y$; as a consequence, we also do not know the gradient of $r$, so we can not minimize $r$ within the classical first order oracle model. On the other hand, we can treat the $\nabla R_i(x)$'s as unbiased estimators of $\nabla r$.

\clearpage


