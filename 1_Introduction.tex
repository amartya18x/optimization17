%!TEX root = optimization17.tex

\section{Introduction}\label{sec:intro}
\emph{Speaker: Patrick Rebeschini, 12/10/2017.}\\

\label{sec:introduction}

Many problems in statistics and machine learning can be formulated as the problem of computing a solution to:
\begin{align}
	\begin{aligned}
		\text{minimize }\quad   & r(x) := \mathbf{E}\ell(x^T\Phi(W),Y)\\
		\text{subject to }\quad & x\in \X,
	\end{aligned}
	\label{def:mainproblem}
\end{align}
given only data in the form of $m$ i.i.d.\ samples $(W_1,Y_1),\ldots,(W_m,Y_m) \sim (W,Y) \in \mathcal{W}\times\mathcal{Y}$, i.e., without knowledge of the distribution of $(W,Y)$ (in particular, without knowledge of the function $r$ that we want to minimize!). Here, the function $\Phi:\mathcal{W}\rightarrow\mathbb{R}^n$ (known) is a feature map, the function $x \in \mathcal{X}\subseteq\mathbb{R}^n \rightarrow x^T\Phi(W)$ is a linear predictor for the label $Y$ (the domain/constraint set $\mathcal{X}$ is known), the function $\ell:\mathbb{R}\times \mathbb{R} \rightarrow \mathbb{R}$ (known) is a loss function characterizing the penalty in committing a wrong prediction, and $r(x)$ is the \emph{test cost}  or \emph{expected risk} associated to the parameter $x$. In this setting, one wants to find $x^\star$ that yields the best linear predictor over unseen data using the data in the training set, under the assumption that the unseen data share the same (unknown) distribution of observed samples.

In binary classification, one has $\mathcal{Y}=\{-1,1\}$ and loss functions are typically of the form $\ell(z,y) = \varphi(-zy)$ for a given $\varphi:\R\rightarrow\R$. A few examples are:
\begin{itemize}
\item Zero-One loss (a.k.a.\ the true loss): $\varphi(u) = \mathbf{1}_{u\ge 0}$.
\item Exponential loss: $\varphi(u) = \exp(u)$.
\item Hinge loss: $\varphi(u) = \max\{0,1+u\}$ (giving SVM).
\item Logistic loss: $\varphi(u) = \log_2(1+\exp(u))$.
\end{itemize}
In regression, one has $\mathcal{Y}=\mathbb{R}$, and the typical loss is:
\begin{itemize}
\item Least-squares loss: $\ell(u,y) = (u-y)^2$.
\end{itemize}

Perhaps the most intuitive paradigm for solving problem \eqref{def:mainproblem} is the \emph{empirical risk minimization}, that is, the idea to minimize the \emph{training cost} or \emph{empirical risk}:
\begin{align}
	\begin{aligned}
		\text{minimize }\quad   & R(x) := \frac{1}{m} \sum_{i=1}^m R_i(x)\\
		\text{subject to }\quad & x\in \X,
	\end{aligned}
	\label{def:mainproblem empirical risk}
\end{align}
where $R_i(x) := \ell(x^T\Phi(W_i),Y_i)$.
This approach is motivated by the fact that for each $x\in\mathcal{X}$ the law of large number gives
$
	r(x) \approx R(x)
$
so that one might expect that
$
	x^\star \approx X^\star,
$
where
$
	X^\star \in \argmin_{x\in\mathcal{X}} R(x).
$
This intuition can be made precise, and Proposition \ref{prop:erm} below shows that the estimation error $r(X^\star) - r(x^\star) \ge 0$ is controlled by suprema of random processes.

\begin{proposition}
\label{prop:erm}
We have
$$
	r(X^\star) - r(x^\star)
	\le
	\sup_{x\in\mathcal{X}} ( r(x) - R(x) ) + \sup_{x\in\mathcal{X}} ( R(x) - r(x) ).
$$
\end{proposition}

\begin{proof}
By adding and subtracting terms, we get
\begin{align*}
	r(X^\star) - r(x^\star)
	= r(X^\star) - R(X^\star) + R(X^\star) - R(x^\star) + R(x^\star) - r(x^\star).
\end{align*}
As $X^\star$ is a minimizer of $R$, we have $R(X^\star) - R(x^\star) \le 0$. The proof follows by taking the supremum over $x\in\mathcal{X}$.
\end{proof}

This analysis assumes that one can actually compute an exact minimizer $X^\star$ for the training cost $R$. With a finite amount of computational resources available, however, typically one can only compute an approximate minimizer. Let $\hat X_t\in\mathcal{X}$ denote an approximate solution to $\argmin_{x\in\mathcal{X}} R(x)$ that is computed after $t$ iterations of a given algorithmic procedure. Proceeding as above we have the following result.

\begin{proposition}
\label{prop:erm2}
We have
\begin{align}
	r(\hat X_t) - r(x^\star)
	\le
	\underbrace{\sup_{x\in\mathcal{X}} ( r(x) - R(x) ) + \sup_{x\in\mathcal{X}} ( R(x) - r(x) )}_{\textrm{STATISTICS}}
	+
	\underbrace{R(\hat X_t) - R(X^\star)}_{\textrm{OPTIMIZATION}}.
	\label{bound:stats-opt}
\end{align}
\end{proposition}

\begin{proof}
By adding and subtracting terms, we get
\begin{align*}
	r(\hat X_t) - r(x^\star)
	= r(\hat X_t) - R(\hat X_t) + R(\hat X_t) - R(X^\star) + R(X^\star) - R(x^\star) + R(x^\star) - r(x^\star).
\end{align*}
As $X^\star$ is a minimizer of $R$, we have $R(X^\star) - R(x^\star) \le 0$. The proof follows by taking the supremum over $x\in\mathcal{X}$.
\end{proof}

The problem is now to estimate how close the expected risk of the approximate empirical risk minimizer $r(\hat X_t)$ is to the minimal expected risk $r(x^\star)$ in terms of the number of training samples $m$, the dimension of the feature map $n$, the number of iterates $t$ for the optimization routine, and the other parameters that define the model at hand (for example, the radius of the parameter set $\mathcal{X}$, properties of the loss function $\ell$, and of the feature map $\Phi$).
To this end, as Proposition \ref{prop:erm2} shows, we need to control two random terms: the \emph{STATISTICS} term $\sup_{x\in\mathcal{X}} ( r(x) - R(x) ) + \sup_{x\in\mathcal{X}} ( R(x) - r(x) )$ and the \emph{OPTIMIZATION} term $R(\hat X_t) - R(X^\star)$.

For the most part this reading group will focus on algorithms that can be used to approximately minimize the empirical risk and hence control the \emph{OPTIMIZATION} term in \eqref{bound:stats-opt}, making various assumptions for the loss function $\ell$ and the parameter space $\mathcal{X}$, starting from the fruitful assumption of convexity.\footnote{Convexity is only needed to perform the analysis of the algorithms that we introduce, but it is not needed to run the algorithms themselves; in fact, in practice the algorithms that we define are run also on non-convex problems.} Along with convexity, we state some of the main properties that a generic function $f$ can have that are used to assess the quality of algorithms to minimize $f$. Here we assume that $f$ is differentiable, but analogous notions can be defined for non-differentiable functions using the notion of subgradients. We also mention in brackets equivalent (local) definitions, which apart from $L$-Lipschitz hold when $f$ is twice-differentiable.

\begin{itemize}
	\item Convexity: $f(x) - f(y) \le \nabla f(x)^T(x - y)$ for any $x,y\in\mathbb{R}^n$ ($\nabla^2 f(x) \succcurlyeq 0$ for any $x\in\mathbb{R}^n$).
	\item $L$-Lipschitz: $| f(x) - f(y) | \le L \| x - y \|_2$ for any $x,y\in\mathbb{R}^n$ ($\|\nabla f(x)\|_2 \le L$ for any $x\in\mathbb{R}^n$).
	\item $\beta$-Smoothness: $\| \nabla f(x) - \nabla f(y) \|_2 \le \beta \| x - y \|_2$ for any $x,y\in\mathbb{R}^n$ ($\nabla^2 f(x) \preccurlyeq \beta I$ for any $x\in\mathbb{R}^n$).
	\item $\alpha$-Strong convexity: $f(x) - f(y) \le \nabla f(x)^T(x - y) - \frac{\alpha}{2}\|x - y\|_2^2$ for any $x,y\in\mathbb{R}^n$ ($\nabla^2 f(x) \succcurlyeq \alpha I$ for any $x\in\mathbb{R}^n$).
\end{itemize}

Going back to the losses defined above, it is easy to check that the convex losses below satisfy the following:
\begin{itemize}
\item Exponential loss: $\varphi(u) = \exp(u)$: Lipschitz NO, Smooth NO, strongly-convex NO.
\item Hinge loss: $\varphi(u) = \max\{0,1+u\}$: Lipschitz YES, Smooth NO, strongly-convex NO.
\item Logistic loss: $\varphi(u) = \log_2(1+\exp(u))$: Lipschitz YES, Smooth YES, strongly-convex NO.
\item Least-squares loss: $\ell(u,y) = (u-y)^2$: Lipschitz NO, Smooth YES, strongly-convex YES.
\end{itemize}

Note that as far as the empirical risk minimization goes, what we care about are the (almost sure) properties of the function $x\in\mathcal{X}\rightarrow R(x) := \frac{1}{m} \sum_{i=1}^m R_i(x) = \frac{1}{m} \sum_{i=1}^m \ell(x^T\Phi(W_i),Y_i)$. In particular, if $\mathcal{X}$ and $\Phi$ are bounded, then $x^T\Phi(W_i)$ is bounded and we can restrict the definitions above to hold on a bounded set. Henceforth, we say that a function $x\in \text{Dom}(f)\subseteq\R^n\rightarrow f(x)$ is $L$-Lipschitz if $| f(x) - f(y) | \le L \| x - y \|_2$ for any $x,y\in \text{Dom}(f)$ ($\|\nabla f(x)\|_2 \le L$ for any $x\in\text{Dom}(f)$). Assuming boundedness provides extra flexibility, allowing one to directly use these properties in setting where otherwise they would not apply (take the exponential loss, for instance, which is only Lipschitz on a bounded interval).

Even if we assume boundedness, assuming that the empirical risk $R$ is strongly convex is typically a strong assumption. In fact, note that \emph{for any} $x\in\mathcal{X}$ (that is, regardless of the properties of $\mathcal{X}$) the Hessian
\begin{align*}
	\nabla^2R(x) = \frac{1}{m} \sum_{i=1}^m \frac{\partial^2}{\partial z^2} \ell(x^T\Phi(W_i),Y_i) 
	\, \Phi(W_i)\Phi(W_i)^T\in\mathbb{R}^{n\times n}
\end{align*}
is not invertible if $m < n$, being a sum of $m<n$ rank-1 matrices. So in this case $x\rightarrow\nabla^2R(x)$ can not be strongly convex. In applications where $m\ge n$, the empirical covariance can be invertible, but it is typically the case that the strong convexity parameter $\alpha$ is very small, of order $1/m$, effectively $\alpha \approx 0$ (or close to machine precision) for applications involving large datasets. For this reason, in this reading group we will only focus on results about Lipschitz and smooth functions, which are more natural assumptions for the empirical risk $R$, as we now show.

First, note that if we assume that $\ell$ is $L_{\textrm{loss}}$-Lipschitz in the first coordinate, namely, $z\rightarrow \ell(z,y)$ is $L_\ell$-Lipschitz for any $y\in\mathcal{Y}$, and that the feature map $\Phi$ is bounded in $\ell_2$, namely, $\|\Phi\|_2\le G$, then $R$ is $GL_\ell$-Lipschitz:
$$
	| R(x) - R(y) | 
	\le \frac{1}{m} \sum_{i=1}^m | \ell(x^T\Phi(W_i),Y_i)-\ell(y^T\Phi(W_i),Y_i) |
	\le \frac{L_\ell}{m} \sum_{i=1}^m \| (x-y)^T\Phi(W_i)) \|_2
	\le GL_\ell \|x-y\|_2,
$$
where for the last inequality we used Cauchy-Schwarz. Second, note that if we assume that $\ell$ is $\beta_\ell $-Lipschitz in the first coordinate, namely, $z\rightarrow \ell(z,y)$ is $\beta_\textrm{loss}$-Lipschitz for any $y\in\mathcal{Y}$, and that $\|\Phi\|_2\le G$, then $R$ is $G^2\beta_\textrm{loss} $-smooth:
\begin{align*}
	\| \nabla R(x) - \nabla R(y) \|_2 
	&\le \frac{1}{m} \sum_{i=1}^m \bigg\| \bigg(\frac{\partial}{\partial z} \ell(x^T\Phi(W_i),Y_i)-\frac{\partial}{\partial z}\ell(y^T\Phi(W_i),Y_i)\bigg) \Phi(W_i) \bigg\|_2\\
	&\le \frac{1}{m} \sum_{i=1}^m \beta_\ell  
	| (x-y)^T\Phi(W_i) |
	\| \Phi(W_i) \|_2
	\le G^2\beta_\ell \|x-y\|_2.
\end{align*}

Before we embark on the adventure of minimizing the empirical risk and bound the \emph{OPTIMIZATION} term in \eqref{bound:stats-opt}, we should try to develop some basic understanding of what to expect from the behavior of the \emph{STATISTICS} term as well. This understanding will give us insights on the type of precision that we should strive for to control the \emph{OPTIMIZATION} term. To this end, we now give a brief detour on statistical learning theory and derive an explicit bound in the case when the loss function $\ell$ is Lipschitz (no assumption on convexity is made for this result), the feature map is bounded, and also the parameter space $\mathcal{X}$ is bounded. The proof is instructive and relies on classical machinery of concentration inequalities and Rademacher complexity.

\subsection{Statistics term}
\label{sec:Statistical learning theory}
Statistical learning theory focuses on understanding how the estimation error $r(X^\star) - r(x^\star)$ converges to zero as a function of the sample size $m$, the loss function $\ell$, the parameter space $\mathcal{X}$, and possibly some properties of the distribution of $\Phi(W)$ and $Y$ (recall that the distribution of the data is not known). We will prove the following results, inspired by the presentation in \cite{bach}.

\begin{proposition}[Mean]
\label{prop:Lip-stats mean}
Let $z\rightarrow \ell(z,y)$ be $L_\ell$-Lipschitz for any $y\in\mathcal{Y}$, $\sup_{x\in\mathcal{X}}\|x\|_2 \le B$ and $\|\Phi(W)\|_2\le G$ a.s.. Then,
$$
	\E[\textrm{STATISTICS}] = \E \sup_{x\in\mathcal{X}} ( r(x) - R(x) ) + \E \sup_{x\in\mathcal{X}} ( R(x) - r(x) )
	\le 4\frac{BGL_\ell}{\sqrt{m}}.
$$
\end{proposition}

\begin{proposition}[High probability]
\label{prop:Lip-stats}
Let $z\rightarrow \ell(z,y)$ be $L_\ell$-Lipschitz for any $y\in\mathcal{Y}$, $\sup_{x\in\mathcal{X}}\|x\|_2 \le B$ and $\|\Phi(W)\|_2\le G$ a.s.. Then, with probability at least $1-\delta$ we have
$$
	\textrm{STATISTICS} = \sup_{x\in\mathcal{X}} ( r(x) - R(x) ) + \sup_{x\in\mathcal{X}} ( R(x) - r(x) )
	\le 2\frac{\ell_0+BGL_\ell}{\sqrt{m}}\left(2 + \sqrt{2\log \frac{1}{\delta}} \right),
$$
where $\ell_0:=\sup_{y\in\mathcal{Y}} | \ell(0,y) |$.
\end{proposition}

The quality of the bound in Proposition \ref{prop:Lip-stats} is typical in statistical learning theory. In particular, it is typically the case that the \emph{slow} rate $O(1/\sqrt{m})$ can not be improved unless other assumptions are taken into consideration that allow to prove the \emph{fast} rate $O(1/m)$. These upper bounds suggest\footnote{Caveats are in order: after all the result in \eqref{bound:stats-opt} is only an upper-bound, and it is often the case that in practice one does not know the constants sitting in front of the error bounds that one can prove for both the \emph{OPTIMIZATION} term and the \emph{STATISTICS} term, i.e., the constants $G,L,R$ in our case.} that we only need to approximate the \emph{OPTIMIZATION} term up to precision $O(1/\sqrt{m})$ (resp. $O(1/m)$).

\subsubsection{Bound on the mean}
\label{sec:Bound on the mean}
We prove Proposition \ref{prop:Lip-stats mean}. An important tool to bound the mean of a supremum of a random process is the Rademacher complexity.
The Rademacher complexity of a set is defined as follows.
\begin{definition}
\label{def:Rademacher}
The Rademacher complexity of a set $T\subseteq\R^m$ is
$$
	\mathcal{R}(T) := \E \sup_{t\in T} \frac{1}{m} \sum_{i=1}^m \varepsilon_i t_i,
$$
where $\varepsilon_1,\ldots,\varepsilon_m$ are i.i.d.\ random variables uniform in $\{-1,1\}$.
\end{definition}

The quantity $\mathcal{R}(T)$ is a measure of how complex the set $T$ is, as $\sup_{t\in T} \sum_{i=1}^m \varepsilon_i t_i$ describes how well elements in $T$ can replicate the sign pattern of a random signal $\varepsilon=(\varepsilon_1,\ldots,\varepsilon_m)$. One way of seeing this is to restrict to the case $T\subseteq [-1,1]^n$. If $T= [-1,1]^n$ then $\mathcal{R}(T)=1$, as for any realization of the signal $\varepsilon$ we can find $t\in T$ that has its same sign pattern. More generally, if $T\subseteq[-1,1]^n$ with $k$-sparse components, then $\mathcal{R}(T)=k/m$. 

One reason why the Rademacher complexity is a useful notion is given by the following lemma, that shows how this quantity behaves with respect to composition with Lipschitz functions.

\begin{lemma}[Contraction property]
\label{lem:contraction}
For each $i\in\{1,\ldots,m\}$, let $\varphi_i:\R\rightarrow\R$ be a $L$-Lipschitz function. Let $(\varphi_1,\ldots,\varphi_m) \circ T=\{ (\varphi_1(t_1),\ldots,\varphi_m(t_m))^T : t\in T \}$. Then,
$$
	\mathcal{R}((\varphi_1,\ldots,\varphi_m) \circ T)
	\le 
	L 
	\mathcal{R}(T),
$$
or, explicitly,
$$
	\E\sup_{t\in T}\sum_{i=1}^m \varepsilon_i \varphi_i(t_i)
	\le
	L \E\sup_{t\in T}\sum_{i=1}^m \varepsilon_i t_i. 
$$
\end{lemma}

\begin{proof}
First, note that for $S\subseteq\R^2$ and $\varphi:\R\rightarrow\R$ $1$-Lipschitz we have
\begin{align*}
	\sup_{s\in S} (s_1+\varphi(s_2)) + \sup_{s\in S} (s_1-\varphi(s_2))
	&= \sup_{s,s'\in S} (s_1+s_1'+\varphi(s_2)-\varphi(s_2'))
	= \sup_{s,s'\in S} (s_1+s_1'+|\varphi(s_2)-\varphi(s_2')|)\\
	&\le \sup_{s,s'\in S} (s_1+s_1'+|s_2-s_2'|)
	= \sup_{s,s'\in S} (s_1+s_1'+s_2-s_2')\\
	&= \sup_{s\in S} (s_1+s_2) + \sup_{s\in S} (s_1-s_2).
\end{align*}
Then, if the functions $\varphi_i$'s are $1$-Lipschitz we have, for any $k\in\{1,\ldots,m\}$,
\begin{align*}
	\E\sup_{t\in T}\sum_{i=1}^m \varepsilon_i \varphi_i(t_i)
	&=
	\E\E\bigg[\sup_{t\in T}\sum_{i=1}^m \varepsilon_i \varphi_i(t_i)\bigg| \varepsilon_1,\ldots,\varepsilon_{i-1},\varepsilon_{i+1},\ldots,\varepsilon_{m}\bigg]\\
	&=
	\frac{1}{2}
	\E 
	\bigg[
	\sup_{t\in T}
	\bigg(
	\sum_{i\neq k} \varepsilon_i \varphi_i(t_i)
	+ \varphi_k(t_k) 
	\bigg)
	+ 
	\sup_{t\in T}
	\bigg(\sum_{i\neq k} \varepsilon_i \varphi_i(t_i)
	- \varphi_k(t_k)
	\bigg)
	\bigg]\\
	&\le
	\frac{1}{2}
	\E 
	\bigg[
	\sup_{t\in T}
	\bigg(
	\sum_{i\neq k} \varepsilon_i \varphi_i(t_i)
	+ t_k
	\bigg)
	+ 
	\sup_{t\in T}
	\bigg(\sum_{i\neq k} \varepsilon_i \varphi_i(t_i)
	- t_k
	\bigg)
	\bigg]\\
	&=
	\E 
	\sup_{t\in T}
	\bigg(
	\sum_{i\neq k} \varepsilon_i \varphi_i(t_i)
	+ \varepsilon_k t_k
	\bigg),
\end{align*}
and by iterating one finds
$$
	\E\sup_{t\in T}\sum_{i=1}^m \varepsilon_i \varphi_i(t_i)
	\le
	\E\sup_{t\in T}\sum_{i=1}^m \varepsilon_i t_i. 
$$
If the functions $\varphi_i$'s are $L$-Lipschitz, then clearly,
$$
	\E\sup_{t\in T}\sum_{i=1}^m \varepsilon_i \varphi_i(t_i)
	=
	L
	\E\sup_{t\in T}\sum_{i=1}^m \varepsilon_i \frac{\varphi_i(t_i)}{L}
	\le
	L \E\sup_{t\in T}\sum_{i=1}^m \varepsilon_i t_i. 
$$
\end{proof}

Given the contraction property for the Rademacher complexity of a set, we can derive the following result.
\begin{proposition}
\label{prop:radamacher}
Let $z\rightarrow \ell(z,y)$ be $L_\ell$-Lipschitz for any $y\in\mathcal{Y}$. Then,
$$
	\mathbf{E}\sup_{x\in\mathcal{X}} ( r(x) - R(x) )
	\le 2\mathbf{E}\sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m \varepsilon_i R_i(x)
	\le
	2L_\ell\mathbf{E} \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i x^T\Phi(W_i),
$$
where $\varepsilon_1,\ldots,\varepsilon_n$ are i.i.d.\ random variables uniform in $\{-1,1\}$, independent of $(W_1,Y_1),\ldots,(W_m,Y_m)$. 
\end{proposition}

\begin{proof}
The proof uses the standard machinery of symmetrization. Let $Z_i=(W_i,Y_i)$ for any $i\in\{1,\ldots,m\}$. Let $Z_1',\ldots,Z_m'$ be an independent copy of the data $Z_1,\ldots,Z_m$, and let $R_i'(x) := \ell(x^T\Phi(W_i'),Y_i')$ for each $i\in\{1,\ldots,m\}$. Using that $r(x)=\mathbf{E}R_i'(x)=\mathbf{E}[R_i'(x)|Z_1,\ldots,Z_m]$, we have
\begin{align*}
	\mathbf{E}\sup_{x\in\mathcal{X}} \bigg( r(x) - \frac{1}{m}\sum_{i=1}^m R_i(x) \bigg)
	&=
	\mathbf{E}\sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m \mathbf{E} [ R_i'(x) - R_i(x) ) | Z_1,\ldots,Z_m]\\
	&\le
	\mathbf{E}\mathbf{E}\bigg[\sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m ( R_i'(x) - R_i(x)) \bigg| Z_1,\ldots,Z_m\bigg]\\
	&=
	\mathbf{E}\sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m ( R_i'(x) - R_i(x) )\\
	&=
	\mathbf{E}\sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m \varepsilon_i (R_i'(x) - R_i(x)) \quad \text{by symmetrization}\\
	&\le
	2\mathbf{E}\sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m \varepsilon_i R_i(x)
	= 2\mathbf{E} \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i \ell(x^T\Phi(W_i),Y_i).
\end{align*}
Conditioning on $Z_1,\ldots,Z_m$ we can apply the contraction property for the Rademacher complexity of a set,
applying Lemma \ref{lem:contraction} with the choice $\varphi_i(z)=\ell(z,Y_i)$ and $T=\{(x^T\Phi(W_1),\ldots,x^T\Phi(W_m))^T : x\in\mathcal{X}\}$:
\begin{align}
	\mathbf{E} \bigg[ \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i \ell(x^T\Phi(W_i),Y_i) \bigg| Z_1,\ldots,Z_m \bigg]
	\le
	L_\ell\mathbf{E} \bigg[ \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i x^T\Phi(W_i) \bigg| Z_1,\ldots,Z_m \bigg],
	\label{boundEmpiricalRad}
\end{align}
and the result follows by taking the expectation.
\end{proof}

\begin{remark}[Emipirical Rademacher complexity]
\label{rem:empiricalRad}
The quantity
$$
	\mathbf{E} \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i x^T\Phi(W_i)
$$
on the right hand side of the bound in Proposition \ref{prop:radamacher} can be interpreted as the expected value of the Rademacher complexity of a random set. In fact, by conditioning on the data $W_1,\ldots,W_m$ we get
$$
	\mathbf{E} \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i x^T\Phi(W_i)
	=
	\mathbf{E}
	\mathbf{E} \bigg[\sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i x^T\Phi(W_i) \bigg| W_1,\ldots,W_m\bigg]
	=
	\mathbf{E}
	\mathbf{E} \bigg[\sup_{t\in T_{W_1,\ldots,W_m}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i t_i \bigg| W_1,\ldots,W_m\bigg],
$$
where
$
	T_{W_1,\ldots,W_m}
	:= \{ (x^T\Phi(W_1),\ldots,x^T\Phi(W_m))^T : x\in\mathcal{X} \}.
$
The Rademacher complexity that we get when we condition on the data, as at the end of the proof of Proposition \ref{prop:radamacher}, is typically referred to as the \emph{empirical} Rademacher complexity.
\end{remark}

We now present a result to control the Rademacher complexity when we have boundedness assumptions with respect to the Euclidean norm.

\begin{proposition}
\label{bound:rad}
Let $\sup_{x\in\mathcal{X}}\|x\|_2 \le B$ and $\|\Phi(W)\|_2\le G$ a.s.. Then,
$$
	\mathbf{E} \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i x^T\Phi(W_i)
	\le 
	\frac{BG}{\sqrt{m}}.
$$
\end{proposition}

\begin{proof}
We have
\begin{align*}
	\mathbf{E} \sup_{x\in\mathcal{X}} \frac{1}{m}\sum_{i=1}^m\varepsilon_i x^T\Phi(W_i)
	&\le \sup_{x\in\mathcal{X}} \| x \|_2
	\mathbf{E} \bigg\|\frac{1}{m}\sum_{i=1}^m\varepsilon_i \Phi(W_i)\bigg\|_2 \quad \text{by Cauchy-Schwarz}\\
	&\le B\mathbf{E} \sqrt{\bigg\|\frac{1}{m}\sum_{i=1}^m\varepsilon_i \Phi(W_i)\bigg\|_2^2}
	\le B \sqrt{ \mathbf{E} \bigg\|\frac{1}{m}\sum_{i=1}^m\varepsilon_i \Phi(W_i)\bigg\|_2^2} \quad \text{by Jensen's inequality}\\
	&= \frac{B}{m} \sqrt{ \mathbf{E} \sum_{j=1}^n \bigg(\sum_{i=1}^m \varepsilon_i \Phi(W_i)_j\bigg)^2 } 
	= \frac{B}{m} \sqrt{ \mathbf{E} \sum_{j=1}^n \sum_{i=1}^m (\varepsilon_i \Phi(W_i)_j)^2 } \quad \text{by the independence of the $\varepsilon$'s}\\
	&= \frac{B}{m} \sqrt{ \mathbf{E}  \sum_{i=1}^m \| \Phi(W_i)\|^2_2 }
	\le \frac{G B}{\sqrt{m}} \quad \text{as $\|\Phi(W)\|_2\le G$ a.s.}
\end{align*}
\end{proof}

Proposition \ref{prop:radamacher} and Proposition \ref{bound:rad} immediately yields the following result, from which Proposition \ref{prop:Lip-stats mean} follows immediately.
\begin{proposition}
\label{prop:expectationsupremum}
Let $z\rightarrow \ell(z,y)$ be $L_\ell$-Lipschitz for any $y\in\mathcal{Y}$, $\sup_{x\in\mathcal{X}}\|x\|_2 \le B$ and $\|\Phi(W)\|_2\le G$ a.s.. Then,
$$
	\mathbf{E}\sup_{x\in\mathcal{X}} ( r(x) - R(x) )
	\le
	2\frac{BGL_\ell}{\sqrt{m}}.
$$
\end{proposition}

\subsubsection{Bound with high probability}
\label{sec:high probability}
We prove Proposition \ref{prop:Lip-stats}. Let us consider the following quantity:
\begin{align}
	h(Z_1,\ldots,Z_m) := \sup_{x\in\mathcal{X}} ( r(x) - R(x) ),
	\label{def:h}
\end{align}
where we use the notation $Z_i := (W_i,Y_i)$. Proposition \ref{prop:expectationsupremum} yields a bound on $\E h(Z_1,\ldots,Z_m)$. A classical way to derive bounds in high probability is to use concentration inequalities, which are tools in probability theory to bound the deviation of a function of many i.i.d.\ random variables from its mean. One of the classical concentration inequalities is the Bounded Difference, of which we now state a simple version.

\begin{proposition}[Bounded Difference Inequality]
\label{prop:BDI}
Let $Z_1,\ldots,Z_m\in\mathcal{Z}$ be $m$ i.i.d.\ random variables, and let $h:\mathcal{Z}^m\rightarrow\mathbb{R}$ be a function satisfying, for every $k\in\{1,\ldots,m\}$ and for all $z_1,\ldots,z_m,z_k'\in\mathcal{Z}$:
$$
	| h(z_1,\ldots,z_{k-1},z_k,z_{k+1},\ldots,z_m) - h(z_1,\ldots,z_{k-1},z_k',z_{k+1},\ldots,z_m) | \le c.
$$
Then,
$$
	\mathbf{P}(h(Z_1,\ldots,Z_m) \ge \mathbf{E}h(Z_1,\ldots,Z_m) + t ) \le \exp\bigg(-\frac{2t^2}{m c^2}\bigg),
$$
or, equivalently, with probability at least $1-\delta$ we have
$$
	h(Z_1,\ldots,Z_m) \le \mathbf{E}h(Z_1,\ldots,Z_m) + c\sqrt{\frac{m}{2}\log\frac{1}{\delta}}.
$$
\end{proposition}
To apply the Bounded Difference inequality to the function $h$ defined in \eqref{def:h}, we need to find $c$ that satisfy the assumption of Proposition \ref{prop:BDI} to control the magnitude of the difference in the function $h$ upon changing only one coordinate. The next proposition shows how to do this we have boundedness assumptions with respect to the Euclidean norm.
\begin{proposition}
\label{prop:c}
Let $h$ be the function defined in \eqref{def:h}. Let $z\rightarrow \ell(z,y)$ be $L_\ell$-Lipschitz for any $y\in\mathcal{Y}$, $\sup_{x\in\mathcal{X}}\|x\|_2 \le B$ and $\|\Phi(W)\|_2\le G$ a.s.. Then, $c\in\mathbb{R}$ satisfying the requirement of the Bounded Difference inequality is given by
$$
	c = \frac{2}{m} \bigg(\ell_0 + BGL_\ell \bigg),
$$
where $\ell_0:=\sup_{y\in\mathcal{Y}} | \ell(0,y) |$.
\end{proposition}

\begin{proof}
Fix $k\in\{1,\ldots,m\}$ and let $z=(z_1,\ldots,z_{k-1},z_k,z_{k+1},\ldots,z_m)$ and $z'=(z_1,\ldots,z_{k-1},z'_k,z_{k+1},\ldots,z_m)$. Then, 
\begin{align*}
	| h(z) - h(z') |
	= \bigg| \sup_{x\in\mathcal{X}} \bigg( r(x)- \frac{1}{m} \sum_{i=1}^m R^z_i(x) \bigg)
	- \sup_{x\in\mathcal{X}} \bigg( r(x)- \frac{1}{m} \sum_{i=1}^m R^{z'}_i(x) \bigg) \bigg|,
\end{align*}
where $R^z_i(x) := \ell(x^T\Phi(w_i),y_i)$. If $h(z) - h(z') \ge 0$ and we let $\tilde x\in\mathcal{X}$ be the maximizer of $\sup_{x\in\mathcal{X}} ( r(x)- \frac{1}{m} \sum_{i=1}^m R^z_i(x) )$ (note that the supremum is attained by the Extreme Value Theorem), we have
\begin{align*}
	h(z) - h(z') 
	&= \bigg( r(\tilde x)- \frac{1}{m} \sum_{i=1}^m R^z_i(\tilde x) \bigg) - \sup_{x\in\mathcal{X}} \bigg( r(x)- \frac{1}{m} \sum_{i=1}^m R^{z'}_i(x) \bigg)\\
	&\le \bigg( r(\tilde x)- \frac{1}{m} \sum_{i=1}^m R^z_i(\tilde x) \bigg) - \bigg( r(\tilde x)- \frac{1}{m} \sum_{i=1}^m R^{z'}_i(\tilde x) \bigg)\\
	&= \frac{1}{m} ( R^z_k(\tilde x) - R^{z'}_k(\tilde x) )
	\le \frac{2}{m} \sup_{x\in\mathcal{X},z\in\mathcal{Z}} | R^z_k(x) |.
\end{align*}
Proceeding analogously in the case $h(z) - h(z') \le 0$, we finally find
$$
	| h(z) - h(z') | \le \frac{2}{m} \sup_{x\in\mathcal{X},z\in\mathcal{Z}} | R^z_k(x) |
	= \frac{2}{m} \sup_{x\in\mathcal{X},w\in\mathcal{W},y\in\mathcal{Y}} | \ell(x^T\Phi(w),y) |.
$$
Using, that $z\rightarrow \ell(z,y)$ is $L_\ell$-Lipschitz for any $y\in\mathcal{Y}$ and Cauchy-Schwarz we get
$$
	|\ell(x^T\Phi(w),y)| \le | \ell(0,y) | + |\ell(x^T\Phi(w),y) - \ell(0,y)|
	\le | \ell(0,y) | + L_\ell\|x^T\Phi(w)\|_2
	\le | \ell(0,y) | + L_\ell\|x\|_2\|\Phi(w)\|_2.
$$
As $\|\Phi(W)\|_2\le G$ a.s.\ and $\sup_{x\in\mathcal{X}}\|x\|_2 \le B$ we have
$
	|\ell(x^T\Phi(w),y)|
	\le | \ell(0,y) | + BGL_\ell,
$
so that
$$
	| h(z) - h(z') | \le \frac{2}{m} \bigg(\sup_{y\in\mathcal{Y}} | \ell(0,y) | + BGL_\ell \bigg).
$$
\end{proof}

The proof of Proposition \ref{prop:Lip-stats} follows by putting everything together, namely, Proposition \ref{prop:BDI}, Proposition \ref{prop:c}, and Proposition \ref{prop:expectationsupremum}, noting that the bounds in high probability that we have obtained hold \emph{simultaneously} for $\sup_{x\in\mathcal{X}} ( r(x) - R(x) )$ and $\sup_{x\in\mathcal{X}} ( R(x) - r(x) )$. Here are the details.

\begin{proof}[Proof of Proposition \ref{prop:Lip-stats}]
Using, respectively, Proposition \ref{prop:BDI} and Proposition \ref{prop:expectationsupremum}, we have that with probability at least $1-\delta$ the following holds:
$$
	\sup_{x\in\mathcal{X}} ( r(x) - R(x) ) \le 
	\mathbf{E}\bigg[\sup_{x\in\mathcal{X}} ( r(x) - R(x) )\bigg] + c\sqrt{\frac{m}{2}\log\frac{1}{\delta}}
	\le \frac{2BGL_\ell}{\sqrt{m}} + c\sqrt{\frac{m}{2}\log\frac{1}{\delta}},
$$
with $c = \frac{2}{m} (\ell_0 + BGL_\ell )$ by Proposition \ref{prop:c}, which yields
$$
	\mathbf{P}\bigg(\sup_{x\in\mathcal{X}} ( r(x) - R(x) ) \le c'\bigg) \ge 1-\delta,
$$
where $c':=(\ell_0+BGL_\ell)(2 + \sqrt{2\log(1/\delta)})/\sqrt{m}$. As the bounds we have derived holds also for $\sup_{x\in\mathcal{X}} ( R(x) - r(x) )$, we have
$$
	\mathbf{P}\bigg(\sup_{x\in\mathcal{X}} ( r(x) - R(x) ) + \sup_{x\in\mathcal{X}} ( R(x) - r(x) ) \le 2c' \bigg)
	\ge \mathbf{P}\bigg(\bigg\{\sup_{x\in\mathcal{X}} ( r(x) - R(x) ) \le c' \bigg\} \bigcap \bigg\{\sup_{x\in\mathcal{X}} ( R(x) - r(x) ) \le c' \bigg\} \bigg) \ge 1-\delta.
$$
\end{proof}


\subsection{Overview of the reading group}

Given the analysis of the previous section, we can now describe the plan for the reading group this term.
While we motivated our interest in optimization techniques by looking at the classical setting of supervised machine learning and empirical risk minimization, only in Week 8 (hopefully!) we will see how to develop specialized algorithms to minimize $R$ (that is, algorithms that can take advantage of the specific structure of $R$). In fact, most of our time will be devoted to reviewing classical algorithms to optimize a \emph{general} convex function $f$ over a convex set $\mathcal{X}$, namely, to solve:
\begin{align*}
	\begin{aligned}
		\text{minimize }\quad   & f(x)\\
		\text{subject to }\quad & x\in\mathcal{X}.
	\end{aligned}
\end{align*}


Below is the current plan of what we aim to cover, along with references to the corresponding literature.\\

\fbox{\begin{minipage}{46em}
\textbf{IMPORTANT NOTE FOR SPEAKERS.}\\The results that are quoted in bold are the ones we will be focusing on. When presenting your section and typing up notes, focus only on covering these results, along with introducing the quantities/proofs the are needed. You might comment on the other results if you have time, or just to give an overview of the bigger picture. In other words, instead of trying to cover as much as possible content-wise, try to cover as little as possible with as much explanation as possible (pictures, intuition, etc.).\\
As we move along in the reading group, the plan below might change. Please make sure that you have the latest version of these lecture notes before you prepare your section.
\end{minipage}}


\subsubsection*{Week 2: Convexity. Black-box model. Projected gradient descent methods.}
Readings: Parts of Chapter 1 and Chapter 3 in \cite{bubeck}. Details are below.\\

We introduce convexity, and some of its basic properties, such as the existence of subgradients (\textbf{Proposition 1.1 in \cite{bubeck}}) and the first order optimality condition (\textbf{Proposition 1.3 in \cite{bubeck}}).

We introduce the black-box model of computation, where we assume that the constraint set $\mathcal{X}\subseteq\mathbb{R}^n$ is known and the objective function $f$ is unknown but can be accessed through queries to first order oracles: given $x\in\mathcal{X}$, a first order oracle yields back a subgradient of $f$ at $x$.

We study how the different assumptions of $L$-Lipschitz, $\beta$-Smoothness, and $\alpha$-Strong convexity, lead to different convergence rates for projected subgradient descent methods. Upon different conditions, different projected subgradient descent methods achieve the following rates:
\begin{center}
 \begin{tabular}{|c | c | c|}
 \hline
 & $L$-Lipschitz & $\beta$-smooth\\
 \hline
 Convex & $O(BL/\sqrt{t})$ & $O(B^2\beta/t)$\\
 \hline
 $\alpha$-strongly convex & $O(L^2/(\alpha t))$ & $O(B^2e^{-t\alpha/\beta})$\\
 \hline
\end{tabular}
\end{center}
where $B$ is the radius of the Euclidean ball that contains $\mathcal{X}$, namely, $\sup_{x\in\mathcal{X}}\|x\|_2 \le B$.
As we discuss earlier, for applications in machine learning the assumption of $\alpha$-strong convexity is typically too strong. This is why we only prove the rates for the convex case, both for $L$-Lipschitz and for $\beta$-Smooth. For completeness, here is where to find all the results mentioned above:
\begin{itemize}
	\item Convex \& $L$-Lipschitz: $O(BL/\sqrt{t})$ (\textbf{Theorem 3.2 in \cite{bubeck}}).
	\item Convex \& $\beta$-Smoothness: $O(B^2\beta/t)$  (\textbf{Theorem 3.7 and in \cite{bubeck}}).
	\item $\alpha$-Strong convexity \& $L$-Lipschitz: $O(L^2/(\alpha t))$ (Theorem 3.9 and in \cite{bubeck}).
	\item $\alpha$-Strong convexity \& $\beta$-Smoothness: $O(B^2\exp(-t\alpha/\beta))$ (Theorem 3.10 and in \cite{bubeck}).
\end{itemize}	


These results are sometimes called \emph{dimension-free}, since as presented the rates do not depend explicitly on the ambient dimension $n$. However, the dependence on the dimension enters implicit in the constants: note that $B$ is the radius of the constraint set $\mathcal{X}\subseteq \mathbb{R}^n$, and that the parameters $L$, $\alpha$, and $\beta$, are defined in terms of the Euclidean norm in $\mathbb{R}^n$. The terminology dimension-free is used in \cite{bubeck} to differentiate the rates achieved by subgradient descent methods from the rates obtained by ellipsoid methods (Chapter 2 in \cite{bubeck}), which we will not cover in this reading group.
	
We can also phrase these results in terms of \emph{oracle complexity}, namely, the number of queries to the oracle that are \emph{sufficient} to find an $\varepsilon$-approximate minima of a convex function:
\begin{center}
 \begin{tabular}{|c | c | c|}
 \hline
 & $L$-Lipschitz & $\beta$-smooth\\
 \hline
 Convex & $O(B^2L^2/\varepsilon^2)$ & $O(B^2\beta/\varepsilon)$\\
 \hline
 $\alpha$-strongly convex & $O(L^2/(\alpha \varepsilon))$ & $O((\beta/\alpha)\log{(B^2/\varepsilon)})$\\
 \hline
\end{tabular}
\end{center}

	
\begin{remark}\label{rem:optimization}
To conclude, let us go back to our motivating example of minimizing the empirical risk function $R(x) = \frac{1}{m}\sum_{i=1}^m \ell(x^T\Phi(W_i),Y_i)$. Under the assumptions of Proposition \ref{prop:Lip-stats mean}, $R$ is $GL_\ell$-Lipschitz (see Section \ref{sec:introduction}). Assuming that the loss function $\ell$ is convex, the subgradient descent method then yields:
$$
	\textrm{OPTIMIZATION} := R(\hat X_t) - R(X^\star) \le \frac{BGL_\ell}{\sqrt{t}}.
$$
Here the constants are explicit, and they match the ones in Proposition \ref{prop:Lip-stats mean} and Proposition \ref{prop:Lip-stats} for the \textrm{STATISTICS} term. In particular, following the rationale of optimizing the \emph{OPTIMIZATION} term in \eqref{bound:stats-opt} up to the accuracy given by the \emph{STATISTICS} term in \eqref{bound:stats-opt}, one finds that it suffices to run the algorithm for $t\sim m$ number of steps.
\end{remark}

\subsubsection*{Week 3: Lower bounds for oracle complexity.}
Readings: Parts of Chapter 3 in \cite{bubeck}. Details are below.\\

We see that within the first order oracle model one can prove lower-bounds for the amount of calls to the oracle that are \emph{needed} to achieve a certain accuracy $\varepsilon$.
In the non-smooth case we show that the rates achieved by subgradient descent methods, i.e., $O(BL/\sqrt{t})$ for convex functions and $O(L^2/(\alpha t))$ for $\alpha$-strongly convex functions, can not be improved (\textbf{Theorem 3.13 in \cite{bubeck}; we will only cover the proof of the first statement for convex $L$-Lipschitz functions}).
On the other hand, in the smooth case we prove oracle-complexity lower bounds that are better than the ones achieved by subgradient methods, namely, $O(D^2\beta/t^2)$ for smooth functions (\textbf{Theorem 3.14 in \cite{bubeck}}) and $O(D^2\exp(-t\sqrt{\alpha/\beta}))$ for smooth and strongly convex functions (Theorem 3.15 in \cite{bubeck}), where $D$ is the diameter of $\mathcal{X}$, i.e., $D:=\max_{x,y\in\mathcal{X}}\|x-y\|_2$. To recap, the optimal rates are:
\begin{center}
 \begin{tabular}{|c | c | c|}
 \hline
 & $L$-Lipschitz & $\beta$-smooth\\
 \hline
 Convex & $O(BL/\sqrt{t})$ & $O(D^2\beta/t^2)$\\
 \hline
 $\alpha$-strongly convex & $O(L^2/(\alpha t))$ & $O(D^2e^{-t\sqrt{\alpha/\beta}})$\\
 \hline
\end{tabular}
\end{center}

\subsubsection*{Week 4: Application: Boosting.}
Reading: Parts of Part II, Section 1.4 and Part II, Section 2.2 in \cite{rigollet}.\\

We apply the projected subgradient descent algorithm to solve an important example in machine learning: Boosting. For a given loss function $\varphi$ (recall, $\ell(z,y) = \varphi(-zy)$ in classification), Boosting can be written as the problem:
\begin{align*}
	\begin{aligned}
		\text{minimize }\quad   & r(x) = \mathbf{E} \varphi(-Yx^T\Phi(W))\\
		\text{subject to }\quad & x\in \Delta_n,
	\end{aligned}
\end{align*}
where $\Delta_n := \{x\in [0,1]^n : \sum_{k=1}^n x_k = 1\}$ is the $n$-dimensional probability simplex. Here the feature map $\Phi : \mathcal{W} \rightarrow \{-1,1\}^n$ encodes the prediction of the $n$ base classifiers on the given data point it is applied to, and $x^T\Phi$ is a convex combination of these classifiers. 
Following the error decomposition given in Proposition \ref{prop:erm2}, we show that in the case of Boosting the \emph{STATISTICS} term is $O(\sqrt{\log n/m})$, which only grows logarithmically with the number of base classifiers $n$. This is a nice feature in applications where the number $n$ of base classifiers is very large, possibly exponential.
Hence, one would hope to be able to design an algorithmic procedure that only uses $\log n$ calls to the first order oracle in order to match the accuracy of the \emph{STATISTICS} term to find an approximate solution to the empirical risk minimization problem:
\begin{align*}
	\begin{aligned}
		\text{minimize }\quad   & R(x) = \frac{1}{m} \sum_{i=1}^m \varphi(-Y_ix^T\Phi(W_i))\\
		\text{subject to }\quad & x\in \Delta_m.
	\end{aligned}
\end{align*}
Note that if $\varphi$ is $L_\varphi$-Lipschitz on $[-1,1]$, then the empirical risk $R$ is $L_\varphi\sqrt{n}$-Lipschitz on $[-1,1]$:
\begin{align*}
	|R(x)-R(y)| 
	&\le \frac{1}{m} \sum_{i=1}^m | \varphi(-Y_ix^T\Phi(W_i)) - \varphi(-Y_iy^T\Phi(W_i)) |
	\le 
	\frac{1}{m} \sum_{i=1}^m L_\varphi \| Y_i(x-y)^T\Phi(W_i) \|_2\\
	&\le L_\varphi \| \Phi(W_i) \|_2 \| x-y \|_2
	\le \sqrt{n}L_\varphi \| x-y \|_2.
\end{align*}
Hence, projected subgradient descent yields a rate $O(L_\varphi\sqrt{n/t})$. Imposing the condition $\sqrt{n/t} \lesssim \sqrt{\log n/m}$, we find that $t\gtrsim nm/\log n$, which does \emph{not} scale logarithmically with $n$ as we hoped for!

Boosting is one example where one would like to apply subgradient descent methods on a non-Euclidean space: the probability simplex $\Delta_n$. However, subgradient descent methods are only designed for the Euclidean geometry. Note, in fact, that all the definitions we gave for $L$-Lipschitz, $\beta$-smoothness, and $\alpha$-strong convexity, as well as the bounds for the constraint set $\mathcal{X}$, are expressed in terms of the Euclidean norm $\|\,\cdot\,\|_2$. This motivates the design of a new class of algorithms (in fact, a generalization of subgradient descent) that can adapt to the geometry of the problem at hand, as we will see next.
%Since, as we saw in Section \ref{sec:Statistical learning theory}, the statistical term is often $O(1/\sqrt{m})$ or $O(1/m)$ in some favourable scenarios, to be generous we would like the optimization term to be bounded by $O(1/m)$, which would yield $t \gtrsim L^2 n m^2$.

\subsubsection*{Week 5: Non-Euclidean setting: mirror descent.}
Readings: Section 4.1, 4.2, and 4.3 in \cite{bubeck}. Details are below.\\

We introduce the mirror descent algorithm and show that this algorithm adapts also to non-Euclidean geometries. We say that $f$ is $L$-Lipschitz in some norm $\|\,\cdot\,\|$ if $|f(x) - f(y)| \le L \| x - y \|$ for any $x,y\in\mathbb{R}^n$. We show that for such convex functions mirror descent yields rates that scale like $O(LB/\sqrt{t})$ (\textbf{Theorem 4.2 in \cite{bubeck}}). This allows mirror descent to achieve a rate $O(\sqrt{\log n/t})$ in the example of boosting, where one wants to minimize a function with subgradient bounded in the $\ell_\infty$-norm over the probability simplex (\textbf{Section 4.3 in \cite{bubeck}}), in contrast to the rate $O(\sqrt{n/t})$ achieved by subgradient descent, as shown last time. We also show that in the case of the Euclidean norm, the algorithm is equivalent to projected subgradient descent.

\subsubsection*{Week 6: Acceleration by coupling gradient descent and mirror descent.}
Readings: \cite{linearcoupling}.\\

We show that mirror descent can be coupled with gradient descent to yield an \emph{accelerated} algorithm that can achieve the lower bound for smooth functions that we proved in Week 3. The acceleration technique is linear coupling, from the paper \cite{linearcoupling}.

\subsubsection*{Week 7: Non-Euclidean setting: Frank-Wolfe.}
Readings: Section 3.3 in \cite{bubeck}. Details are below.\\

In many applications, the computational bottleneck in gradient descent methods is given by the projection step on the constraint set $\mathcal{X}$. To address this issue, we study the Frank-Wolfe algorithm (a.k.a.\ conditional gradient descent), which replaces the projection step with a linear optimization over $\mathcal{X}$, which in some cases can be a much simpler problem. For smooth objective functions, the Frank-Wolfe algorithm achieves rate $O(B^2\beta/t)$ (\textbf{Theorem 3.8 in \cite{bubeck}}). While this rate is slow and it does not match the oracle complexity lower bound $O(D^2\beta/t^2)$, the saving in the computational complexity makes this algorithm convenient in some applications. Other advantages of this algorithm over gradient descent methods is that this algorithm can apply to smoothness in any norm ($f$ is $\beta$-smooth in some norm $\|\,\cdot\,\|$ if $\| \nabla f(x) - \nabla f(y) \|_* \le \beta \| x - y \| $ for any $x,y\in\mathbb{R}^n$, where the dual norm $\|\,\cdot\,\|_*$ is defined as $\|g\|_* = \sup_{x\in\mathbb{R}^n:\|x\|\le 1} g^Tx$), and that the algorithm computes sparse iterates. We will discuss a concrete application where these benefits are substantials, the least square regression with structured sparsity (\textbf{in Section 3.3 in \cite{bubeck}}).

\subsubsection*{Week 8: Stochastic oracle model.}
Readings: Section 6.1, 6.2, and 6.3 in \cite{bubeck}.\\

The first order oracle model that we have investigated so far had allowed us to produce a complete theory of convex optimization, in the sense that for various classes of convex functions we can design algorithms with an \emph{oracle complexity} that matches the lower bounds. However, the black-box model does not tell us anything about the \emph{computational complexity}, namely, the number of elementary computations that an algorithm needs to do to solve the problem (indeed, to address this question we should ``open'' the black-box). Going back to the original problem in machine learning that we set to solve, as described in Section \ref{sec:introduction}, we note that the first order oracle model is too general for our needs, and that there might be computational savings to be gained in working with a model of computation that is more fined-tuned to our problem. This consideration motivates us to consider the \emph{stochastic} first order oracle model, where for any point $x\in\mathcal{X}$ the oracle gives back an unbiased estimator of the gradient at $x$. Within this framework, two  approaches have attracted a lot of attention lately, due to the computational savings they yield.

\underline{Multiple passes over the data.}
Within the setting of empirical risk minimization, one notices that we know the \emph{global} structure of the function we want to minimize, namely, $R=\frac{1}{m}\sum_{i=1}^m R_i$. Hence, for instance, given $x\in\mathcal{X}$ we can have access to $\nabla R_i(x)$ for a specific $i\in\{1,\ldots,m\}$, which is something that is not allowed in the oracle model previously discussed where only access to $\nabla R(x)$ is granted. Note that computing $\nabla R(x)$ costs $O(m)$ operations as $R$ is the sum of $m$ terms, while computing $\nabla R_i(x)$ costs $O(1)$.

\underline{Single pass over the data.}
The empirical risk minimization approach described in Section \ref{sec:introduction} has allowed us to break the original problem we care about, namely, problem \eqref{def:mainproblem}, into a \emph{STATISTICS} component and an \emph{OPTIMIZATION} component, see Proposition \ref{prop:erm2}. We show that within the stochastic oracle model there is a way to directly address problem \eqref{def:mainproblem} and yield a bound for $r(\hat X_t) - r(x^\star)$, \emph{combining} both statistics and optimization.
Recall that we do not know the function $r$ as we assume that we do not know the distribution of the random variables $W$ and $Y$; as a consequence, we also do not know the gradient of $r$, so we can not minimize $r$ within the classical first order oracle model. On the other hand, we can treat the $\nabla R_i(x)$'s as unbiased estimators of $\nabla r$.

\clearpage


